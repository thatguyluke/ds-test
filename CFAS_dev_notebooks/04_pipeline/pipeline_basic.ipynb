{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic Pipeline Example\n",
    "\n",
    "This notebook shows how to use basic functions of the Cortex Python SDK pipeline. \n",
    "In this example, see how to modify or enrich datasets to make them suitable for training or modeling. \n",
    "Data is modified in a sequential series of steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Cortex and other required libraries\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from cortex import Cortex\n",
    "\n",
    "# Create a Builder instance\n",
    "cortex = Cortex.local()\n",
    "builder = cortex.builder()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next step, create a data set and populate it from a comma separated values file. A pipeline operates on a dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name_ff = input(\"namespace/dataset name\")\n",
    "\n",
    "data_set = builder.dataset(dataset_name_ff).title('Forest Fire Data')\\\n",
    "    .from_csv('data/ff.sample.csv').build()\n",
    "# Create a pandas DataFrame to view the last few lines of the dataset\n",
    "data_frame = data_set.as_pandas()\n",
    "data_frame.tail(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**validate types**\n",
    "\n",
    "We need to make sure that the types in the columns are what we are expecting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data_frame.dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A dataset can have one or more named pipelines. Each pipeline is a chain of Python functions that transform the dataset.  In the next step, create a pipeline named \"prep\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = data_set.pipeline('prep') # create or retrieve the pipline named 'prep'\n",
    "pipeline.reset() # removes any previous steps or context for this pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One pipeline step can be used to add a new column.\n",
    "\n",
    "This [dataset](http://piano.dsi.uminho.pt/~pcortez/fires.pdf) uses components from the Fire Weather Index to make predictions. One element, the Build Up Index (BUI) is based on a relation of two other columns and is omitted. This step adds that missing element. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(pipeline, df):\n",
    "    cols = [\"DMC\", \"DC\", \"area\", \"X\", \"Y\", \"FFMC\", \"temp\"]\n",
    "    df[cols] = df[cols].apply(pd.to_numeric, errors='coerce')\n",
    "\n",
    "pipeline.add_step(normalize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_bui(pipeline, df):\n",
    "    df['BUI'] = (0.8 * df['DMC'] * df['DC'])/(df['DMC'] + 0.4 * df['DC'])\n",
    "\n",
    "pipeline.add_step(add_bui)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the preceeding code, the pipeline step functions require a pipeline and a dataframe parameter. The [pandas DataFrame](https://pandas.pydata.org/pandas-docs/stable/dsintro.html#dataframe) provides a rich set of functions for operating on table data.\n",
    "\n",
    "A pipeline step may be used to modify a column.\n",
    "\n",
    "The dataset's documentation says that the last column, __area__, is skewed towards zero and should be adjusted logarithmically \"to improve regression results for right-skewed targets\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fix_area(pipeline, df):\n",
    "    df['area'] = df['area'].map(lambda a: math.log1p(a))\n",
    "    \n",
    "pipeline.add_step(fix_area)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running the Pipeline\n",
    "After all the steps are added, you can call `run` on the pipeline. This invokes each of the steps in order and returns a transformed DataFrame instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_frame = pipeline.run(data_frame)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can verify what your pipeline did here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_frame.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here you can visualize your data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_graphs(data_set, data_frame, attribute):\n",
    "\n",
    "    v = data_set.visuals(data_frame)\n",
    "    v.show_corr_heatmap()\n",
    "    v.show_corr(attribute)\n",
    "    v.show_corr_pairs(attribute)\n",
    "    v.show_dist(attribute)\n",
    "    v.show_probplot(attribute)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "draw_graphs(data_set, data_frame, 'area')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets save our pipeline for later use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline.to_camel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_set.save()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Break point**\n",
    "\n",
    "We have added metadata to our local sample dataset, now we want to use this pipeline on the full dataset and move it to cortex remote"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a non-local Builder instance\n",
    "from cortex.builder.dataset_builder import DatasetBuilder\n",
    "cortex = Cortex.client()\n",
    "builder = cortex.builder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to help with making this dataset distinct in class we will use an input generator here for the dataset name.  \n",
    "# This variable will be stored throughout this example.\n",
    "dataset_forestfire = input(\"namespace/dataset name\")\n",
    "    \n",
    "\n",
    "\n",
    "#csv_example_data_set = csv_data_set_builder.from_csv('./data/sample_large.csv').build()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll take a look at the pipeline camel spec here\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline.to_camel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_set = builder.dataset(dataset_forestfire).title('Forest Fire Data')\\\n",
    "    .from_csv('data/forestfires.csv').build()\n",
    "# Create a pandas DataFrame to view the last few lines of the dataset\n",
    "data_frame = data_set.as_pandas()\n",
    "data_frame.tail(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "remote_pipeline = data_set.pipeline('prep') # create or retrieve the pipline named 'prep'\n",
    "remote_pipeline.from_pipeline(pipeline)\n",
    "#pipeline.reset() # removes any previous steps or context for this pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "remote_pipeline.run(data_frame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
